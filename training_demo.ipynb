{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9255d427",
   "metadata": {},
   "source": [
    "# Training demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a2964",
   "metadata": {},
   "source": [
    "Before training the model, we should first import some necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8f96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed3a1f",
   "metadata": {},
   "source": [
    "According to the torchvision documentationï¼š\n",
    "[torchvision documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn)\n",
    "\n",
    "During training, the model expects both the input image tensors and targets (list of dictionary), containing:\n",
    "\n",
    "boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H.\n",
    "\n",
    "labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a13bfd",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f772c4c",
   "metadata": {},
   "source": [
    "So\n",
    "\n",
    "In the \"init\" function:\n",
    "\n",
    "We creates two lists: \"imgs\" and \"anns\". \"imgs\" contains the names of all image files in the \"images_train\" directory, and \"anns\" contains the names of all annotation files in the \"annotations_train\" directory. These lists are sorted in alphabetical order to ensure that each image and its corresponding annotation are matched correctly.\n",
    "\n",
    "In the \"getitem\" function:\n",
    "\n",
    "We load the image and annotation data by the index of the sorted list, and then convert the image and annotation to the tensor type required by the model, the annotation tensors are combined into a dictionary named \"target\", finally, the function returns a tuple containing the loaded image tensor and the \"target\" dictionary which is the required input format for the model.\n",
    "\n",
    "If your data size is small, you can also do some random data augmentation during training by adding a transform function in the getitem method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671f2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, image_set='images_train', annotation_set='annotations_train'):\n",
    "        self.root = root\n",
    "        self.image_set = image_set\n",
    "        self.annotation_set = annotation_set\n",
    "        \n",
    "        # Load all image and annotation files and make sure they are sorted\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, image_set))))\n",
    "        self.anns = list(sorted(os.listdir(os.path.join(root, annotation_set))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load images and annotations\n",
    "        img_path = os.path.join(self.root, self.image_set, self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root, self.annotation_set, self.anns[idx])\n",
    "        img = Image.open(img_path)\n",
    "        ann = np.load(ann_path, allow_pickle='TRUE').item()\n",
    "        \n",
    "        # Convert annotation into torch Tensor\n",
    "        boxes = torch.as_tensor(ann['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(ann['labels'], dtype=torch.int64)\n",
    "        masks = torch.as_tensor(ann['masks'], dtype=torch.uint8)\n",
    "        \n",
    "        # Convert image into torch Tensor\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        img = convert_tensor(img)\n",
    "        \n",
    "        # Put annotations into 'target' which torchvision.models.detection.maskrcnn_resnet50_fpn required\n",
    "        target = {}    \n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['masks'] = masks\n",
    "        \n",
    "        # Return required inputs of model which are input tensor and target\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab5f72",
   "metadata": {},
   "source": [
    "## Define collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebebd8",
   "metadata": {},
   "source": [
    "In this section, we define the collation functions for the data loader, since the default functions are not suitable for our data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc6c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader's collate function\n",
    "def my_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b1bf8",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa3972",
   "metadata": {},
   "source": [
    "Next, we load data from CustomDataset class and split it into train and valid sets, then load both datasets into dataloaders.\n",
    "\n",
    "Due to the limitation of Vram, we set the batch size to 1 in this demo, if you have a GPU with larger Vram, you can try to increase the batch size, and try to use multiple num_workers.\n",
    "\n",
    "The recommended batch size is Vram size/6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384e7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train dataset and valid dataset\n",
    "dataset = CustomDataset('dataset')\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# Load datasets to dataloaders\n",
    "data_loader_train = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, collate_fn=my_collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf8568",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ec378",
   "metadata": {},
   "source": [
    "If we want to fine-tune Mask-RCNN on a custom dataset, we need to modify the model's pre-trained head to fit our data classes.\n",
    "\n",
    "The architecture of the Mask-RCNN model has a classifier (box_predictor) and a mask classifier (mask_predictor), both of which should be adapted to our data classes. So we need to modify both pretrained heads with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aaa4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "def build_model(num_classes):\n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                      aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='DEFAULT',min_size=51, max_size = 640)\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999d370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34663d67",
   "metadata": {},
   "source": [
    "## Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90601b74",
   "metadata": {},
   "source": [
    "After defining the model architecture, we can now build the model with custom classes number and move the model to the GPU.\n",
    "\n",
    "(cpu can also do training, but it will be extremely slow)\n",
    "\n",
    "The default optimizer we're using here is SGD with a default learning rate of 0.0001, but of course you can tweak it as much as you want to make the model fit your data better.\n",
    "\n",
    "The default learning rate is fixed during training, but you can also use lr_scheduler to change your learning rate during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1387e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training device is cuda\n"
     ]
    }
   ],
   "source": [
    "# Set caculating deivce\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"training device is {device}\")\n",
    "\n",
    "# Set classes number (should include background here)\n",
    "num_classes = 2\n",
    "\n",
    "# Get the model using our helper function\n",
    "model = build_model(num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Construct an optimizer\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(model_parameters, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Set a learning rate scheduler (defualt value is a constant learning rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac5c4d",
   "metadata": {},
   "source": [
    "## Define training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ee2c2",
   "metadata": {},
   "source": [
    "In the training function, we create a dictionary to store the losses for each training iteration. The losses including:\n",
    "\n",
    "classifier loss,box regression loss, mask loss, objectness loss, RPN box regression loss,and the sum of these individual losses.\n",
    "\n",
    "If the current epoch is the first epoch, a learning rate scheduler is set up to perform warm-up learning, where the learning rate is gradually increased from a very small value to the actual learning rate specified in the optimizer.\n",
    "\n",
    "We also use the tqdm library to display our training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c568ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    lr_scheduler = None\n",
    "    \n",
    "    # Build a dictionary to recored losses\n",
    "    LOSS = {\n",
    "        'loss_classifier':[],\n",
    "        'loss_box_reg':[],\n",
    "        'loss_mask':[],\n",
    "        'loss_objectness':[],\n",
    "        'loss_rpn_box_reg':[],\n",
    "        'loss_sum':[]\n",
    "    }\n",
    "    \n",
    "    # Warm up the model in the first epoch\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "        \n",
    "    # Training process\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        for i in loss_dict.keys():\n",
    "            LOSS[i].append(loss_dict[i].item())    \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        LOSS['loss_sum'].append(loss_value)\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebea90",
   "metadata": {},
   "source": [
    "## Define validating process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95d423",
   "metadata": {},
   "source": [
    "Just like the training function, we also create a dictionary to store the losses for each validating iteration in validating function.\n",
    "\n",
    "To speed up our validation process, we use torch.no_grad() to make the model evaluate without computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc4d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model, data_loader, device, epoch):\n",
    "    # Build a dictionary to recored losses\n",
    "    LOSS = {\n",
    "        'val_loss_classifier':[],\n",
    "        'val_loss_box_reg':[],\n",
    "        'val_loss_mask':[],\n",
    "        'val_loss_objectness':[],\n",
    "        'val_loss_rpn_box_reg':[],\n",
    "        'val_loss_sum':[]\n",
    "    }\n",
    "    \n",
    "    # Speed up evaluation by not computing gradients\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "        \n",
    "        # Evaluating process\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            for i in loss_dict.keys():\n",
    "                LOSS[\"val_\"+i].append(loss_dict[i].item())    \n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            LOSS['val_loss_sum'].append(loss_value)\n",
    "\n",
    "    return LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443798f4",
   "metadata": {},
   "source": [
    "## Set up some names, logging tool and epoch numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02d7e1",
   "metadata": {},
   "source": [
    "Before training our model, we should set our model name and build a logging tool, and set the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744a7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "model_name = \"mrcnn\"\n",
    "\n",
    "# Build a dictionary to recored losses\n",
    "LOSSES = {\n",
    "        'loss_classifier':[],\n",
    "        'loss_box_reg':[],\n",
    "        'loss_mask':[],\n",
    "        'loss_objectness':[],\n",
    "        'loss_rpn_box_reg':[],\n",
    "        'loss_sum':[],\n",
    "        'val_loss_classifier':[],\n",
    "        'val_loss_box_reg':[],\n",
    "        'val_loss_mask':[],\n",
    "        'val_loss_objectness':[],\n",
    "        'val_loss_rpn_box_reg':[],\n",
    "        'val_loss_sum':[]\n",
    "    }\n",
    "\n",
    "# Declare the checkpoint saving paths\n",
    "PATH = 'checkpoints/' + model_name + '_checkpoint.pt'\n",
    "min_PATH = 'checkpoints/' + model_name + '_checkpoint_min.pt'\n",
    "\n",
    "# Declare a minimum loss value to ensure whether the current epoch has the minimum loss\n",
    "min_loss = None\n",
    "\n",
    "# Set the training epochs number\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe4f74",
   "metadata": {},
   "source": [
    "## Start training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1679cf3",
   "metadata": {},
   "source": [
    "Finally, we can start training our custom mask-rcnn model. You can monitor the training progress of each epoch through the progress bar.\n",
    "\n",
    "After each epoch is trained, There will be a loss curve chart showing the current loss curve for every types of losses. This chart will also be saved in the current directory.\n",
    "\n",
    "The program will automatically save the checkpoints of the model after every 5 epochs, and save the checkpoints with the smallest verification loss during training. You can also modify the code to adjust the frequency of saving checkpoints.\n",
    "\n",
    "After training, the model will be saved. If you want to use the checkpoint with the smallest validation loss as the final model, you can additionally load the checkpoint and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437d9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is training - learning rate = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/28450 [00:05<42:37, 11.11it/s]  /home/sharath/anaconda3/envs/mrcnn/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 13%|â–ˆâ–Ž        | 3628/28450 [04:18<29:27, 14.04it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [1.1851248741149902, 22.416223526000977, 1.843000054359436, 22.416223526000977] for target at index 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is training - learning rate = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train for one epoch and recored train losses\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m LOSSES_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Update the learning rate\u001b[39;00m\n\u001b[1;32m      8\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     26\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 27\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     29\u001b[0m     LOSS[i]\u001b[38;5;241m.\u001b[39mappend(loss_dict[i]\u001b[38;5;241m.\u001b[39mitem())    \n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:95\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m             bb_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(degenerate_boxes\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 95\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAll bounding boxes should have positive height and width.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m Found invalid box \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdegen_bb\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m for target at index \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.10/site-packages/torch/__init__.py:1559\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [1.1851248741149902, 22.416223526000977, 1.843000054359436, 22.416223526000977] for target at index 0."
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "for epoch in range(num_epochs):\n",
    "        print(f\"epoch {epoch} is training - learning rate = {lr_scheduler.get_last_lr()[0]}\")  \n",
    "        # Train for one epoch and recored train losses\n",
    "        LOSSES_train = train_one_epoch(model, optimizer, data_loader_train, device, epoch)\n",
    "        \n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        print(f\"epoch {epoch} is validating\")\n",
    "        # valid for one epoch and recored valid losses\n",
    "        LOSSES_val = val_one_epoch(model, data_loader_val, device, epoch)\n",
    "        \n",
    "        # Draw and save the loss curve\n",
    "        fig = plt.figure(epoch)\n",
    "        plt.suptitle(f\"Training Loss till epoch {epoch}\")\n",
    "        \n",
    "        for i, v in enumerate(LOSSES_train.keys()):\n",
    "            LOSSES[v].append(sum(LOSSES_train[v])/len(LOSSES_train[v]))\n",
    "            plt.xlim([0, epoch])\n",
    "            plt.subplot(3, 2, i+1)\n",
    "            plt.plot(LOSSES[v], label=v, color='b')\n",
    "            plt.title(v)\n",
    "        \n",
    "        for i, v in enumerate(LOSSES_val.keys()):\n",
    "            LOSSES[v].append(sum(LOSSES_val[v])/len(LOSSES_val[v]))\n",
    "            plt.xlim([0, epoch])\n",
    "            plt.subplot(3, 2, i+1)\n",
    "\n",
    "            plt.plot(LOSSES[v], label=v, color='r')\n",
    "            plt.legend(fontsize='xx-small', loc='upper right') \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('losses_curves/' + model_name + \"_\" + str(epoch) + \"_losses_curve.png\", dpi=600)\n",
    "        plt.close(fig)\n",
    "        # plt.show()\n",
    "        \n",
    "        # Print out current train loss and valid loss\n",
    "        print(f\"train loss sum = {sum(LOSSES_train['loss_sum'])/len(LOSSES_train['loss_sum'])}\")\n",
    "        print(f\"valid loss sum = {sum(LOSSES_val['val_loss_sum'])/len(LOSSES_val['val_loss_sum'])}\\n\")\n",
    "        \n",
    "        # If the loss for the current epoch is minimal, save it to the checkpoint\n",
    "        if not min_loss or LOSSES['val_loss_sum'][-1] < min_loss:\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': LOSSES,\n",
    "                    }, min_PATH)\n",
    "            min_loss = LOSSES['val_loss_sum'][-1]\n",
    "        \n",
    "        # Save training datas to the checkpoint every 5 epoch\n",
    "        if (epoch+1)%5 == 0:\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': LOSSES,\n",
    "                    }, PATH)\n",
    "            \n",
    "# Save the final model \n",
    "torch.save(model, 'models/' + model_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d22e1",
   "metadata": {},
   "source": [
    "## Simple evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f36bc",
   "metadata": {},
   "source": [
    "After training, we now have a custom model, we can try to do some simple evaluation through the model.\n",
    "\n",
    "Of course, we need to import some dependencies first and define some functions for prediction and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c544ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors():\n",
    "    colors = []\n",
    "    for i in range(0,255,120):\n",
    "        for j in range(0,255,120):\n",
    "            for k in range(0,255,120):\n",
    "                colors.append((i,j,k))\n",
    "    colors.pop(0)\n",
    "    random.shuffle(colors)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da332da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(mask,color):\n",
    "    color_mask = cv2.cvtColor(mask.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    color_mask[mask > 0] = color\n",
    "    return color_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949338e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img, model, class_name, threshold=0.5):\n",
    "    img_tensor = transforms.ToTensor()(img).to(device,torch.float)\n",
    "    result = model([img_tensor])\n",
    "    scores = list(result[0]['scores'].cpu().detach().numpy())\n",
    "    valid_num = sum(map(lambda x : x > threshold, scores))\n",
    "    masks = (result[0]['masks']>0.5).squeeze().cpu().detach().numpy()[:valid_num]\n",
    "    boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(result[0]['boxes'].detach().cpu().numpy().astype(int))][:valid_num]\n",
    "    classes = [class_name[i] for i in list(result[0]['labels'].cpu().numpy())][:valid_num]\n",
    "    return masks, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ddfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(img, colors, masks, boxes, classes):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(len(masks)):\n",
    "        color = colors[i%len(colors)]\n",
    "        color_mask = colorize(masks[i],color)\n",
    "        img = cv2.addWeighted(img, 1, color_mask, 0.5, 0)\n",
    "        cv2.rectangle(img, boxes[i][0], boxes[i][1], color, 2)\n",
    "        x,y = boxes[i][0]\n",
    "        cv2.putText(img, classes[i], (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b168605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(img_path, model, class_name):\n",
    "    img = cv2.imread(img_path)\n",
    "    masks, boxes, classes = predict(img, model, class_name)\n",
    "    result = post_processing(img, get_colors(), masks, boxes, classes)\n",
    "    plt.figure(1)\n",
    "    plt.imshow(result)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc8744",
   "metadata": {},
   "source": [
    "Now, we can define our custom classes and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "class_name = ['__backgrounf__','person']\n",
    "plot_result(img_path='test.jpg', model=model, class_name=class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
